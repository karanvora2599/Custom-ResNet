{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetSmall(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNetSmall, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        downsample = None\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "layers=[1, 1, 1, 1]\n",
    "modelsmall = ResNetSmall(BasicBlock, layers).to(device)\n",
    "summary(modelsmall, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetMedium(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNetMedium, self).__init__()\n",
    "        self.in_planes = 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        downsample = None\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "modelmedium = ResNetMedium(BasicBlock, layers).to(device)\n",
    "summary(modelmedium, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLarge(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNetLarge, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        downsample = None\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "modellarge = ResNetLarge(BasicBlock, layers).to(device)\n",
    "summary(modellarge, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Defining Transformers for train and test set differently\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomCrop(32, padding=2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean = [0.4914, 0.4822, 0.4465], \n",
    "                                                std = [0.2023, 0.1994, 0.2010])\n",
    "                       ])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, labels, classes, normalize = False):\n",
    "    n_images = len(images)\n",
    "    rows = int(np.sqrt(n_images))\n",
    "    cols = int(np.sqrt(n_images))\n",
    "    fig = plt.figure(figsize = (10, 10))\n",
    "    for i in range(rows*cols):\n",
    "        ax = fig.add_subplot(rows, cols, i+1)       \n",
    "        image = images[i]\n",
    "        if normalize:\n",
    "            image_min = image.min()\n",
    "            image_max = image.max()\n",
    "            image.clamp_(min = image_min, max = image_max)\n",
    "            image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "        ax.set_title(classes[labels[i]])\n",
    "        ax.axis('off')\n",
    "\n",
    "N_IMAGES = 25\n",
    "images, labels = zip(*[(image, label) for image, label in [train_dataset[i] for i in range(N_IMAGES)]])\n",
    "classes = test_dataset.classes\n",
    "plot_images(images, labels, classes, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, criterion, optimizer, scheduler = None, early_stop=None):\n",
    "    learning_rate_tracker = {}\n",
    "    epoch_correct = 0\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, (images, labels) in tqdm(enumerate(data_loader)):\n",
    "        learning_rate_tracker[i] = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels) \n",
    "        running_loss += loss.item()\n",
    "        predicted = torch.max(outputs.data, 1)[1]\n",
    "        epoch_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        if early_stop and i==early_stop:\n",
    "            break\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    \n",
    "    return epoch_correct , running_loss, learning_rate_tracker\n",
    "    \n",
    "    \n",
    "def evaluate(data_loader, model, criterion):\n",
    "    epoch_correct = 0\n",
    "    running_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels) \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            epoch_correct += (predicted == labels).sum().item()\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    return epoch_correct, running_loss, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_min = 7*1e-5\n",
    "lr_max = 1e-2\n",
    "epochs = 30\n",
    "step_size = (len(train_dataset)/64) // 2\n",
    "\n",
    "model = modelsmall\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr_min, momentum=0.9, nesterov=True)\n",
    "scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=lr_min, max_lr=lr_max, step_size_up=step_size, step_size_down=step_size, gamma=0.9999, mode=\"exp_range\", cycle_momentum=False)\n",
    "lr_tracker = {}\n",
    "\n",
    "train_loss_history_small = []\n",
    "train_acc_history_small = []\n",
    "val_loss_history_small = []\n",
    "val_acc_history_small = []\n",
    "y_pred_small = []\n",
    "y_true_small = []\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "    correct, loss, rate_tracker = train(data_loader=train_loader, model=model, criterion=criterion, optimizer=optimizer, scheduler=scheduler)\n",
    "    accuracy = correct / len(train_loader.dataset)\n",
    "    loss = loss / len(train_loader)\n",
    "    train_loss = loss\n",
    "    train_acc_history_small.append(accuracy)\n",
    "    train_loss_history_small.append(loss)\n",
    "    for key in rate_tracker.keys():\n",
    "        lr_tracker[(epoch,key)] = rate_tracker[key]\n",
    "    correct, loss, y_true_small, y_pred_small = evaluate(data_loader = test_loader, model=model, criterion=criterion)\n",
    "    validation_accuracy = correct / len(test_loader.dataset)\n",
    "    validation_loss = loss / len(test_loader)\n",
    "    print(f\"Train Accuracy: {accuracy*100:.2f}%, Train Loss: {train_loss}\")\n",
    "    print(f\"Validation Accuracy: {validation_accuracy*100:.2f}%, Validtion Loss: {validation_loss}\")\n",
    "    if validation_loss < best_valid_loss:\n",
    "        best_valid_loss = validation_loss\n",
    "        torch.save(model.state_dict(), 'ResNetSmall.pt')\n",
    "    val_acc_history_small.append(validation_accuracy)\n",
    "    val_loss_history_small.append(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "plt.title('Dynamic Learning Rate behaviour during the training of ResNetSmall Model')\n",
    "plt.plot(range(len(lr_tracker)), lr_tracker.values())\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "plt.title('Accuracy and Loss Plots for ResNetSmall Model')\n",
    "plt.plot(train_loss_history_small, label='Train Loss')\n",
    "plt.plot(val_loss_history_small, label='Val Loss')\n",
    "plt.plot(train_acc_history_small, label='Train Acc')  \n",
    "plt.plot(val_acc_history_small, label='Val Acc')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "yticks = np.linspace(0, 1.1, num=30)\n",
    "ax.set_yticks(yticks)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_true_small, y_pred_small)\n",
    "fig = plt.figure(figsize = (10, 10));\n",
    "ax = fig.add_subplot(1, 1, 1);\n",
    "ax.set_title('Confusion Matrix for ResNetSmall Model')\n",
    "cm = ConfusionMatrixDisplay(cm, display_labels = classes);\n",
    "cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)\n",
    "plt.xticks(rotation = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_min = 7*1e-5\n",
    "lr_max = 1e-2\n",
    "epochs = 30\n",
    "step_size = (len(train_dataset)/64) // 2\n",
    "\n",
    "model = modelmedium\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr_min, momentum=0.9, nesterov=True)\n",
    "scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=lr_min, max_lr=lr_max, step_size_up=step_size, step_size_down=step_size, gamma=0.9999, mode=\"exp_range\", cycle_momentum=False)\n",
    "lr_tracker = {}\n",
    "\n",
    "train_loss_history_medium = []\n",
    "train_acc_history_medium = []\n",
    "val_loss_history_medium = []\n",
    "val_acc_history_medium = []\n",
    "y_pred_medium = []\n",
    "y_true_medium = []\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "    correct, loss, rate_tracker = train(data_loader=train_loader, model=model, criterion=criterion, optimizer=optimizer, scheduler=scheduler)\n",
    "    accuracy = correct / len(train_loader.dataset)\n",
    "    loss = loss / len(train_loader)\n",
    "    train_loss = loss\n",
    "    train_acc_history_medium.append(accuracy)\n",
    "    train_loss_history_medium.append(loss)\n",
    "    for key in rate_tracker.keys():\n",
    "        lr_tracker[(epoch,key)] = rate_tracker[key]\n",
    "    correct, loss, y_true_medium, y_pred_medium = evaluate(data_loader = test_loader, model=model, criterion=criterion)\n",
    "    validation_accuracy = correct / len(test_loader.dataset)\n",
    "    validation_loss = loss / len(test_loader)\n",
    "    print(f\"Train Accuracy: {accuracy*100:.2f}%, Train Loss: {train_loss}\")\n",
    "    print(f\"Validation Accuracy: {validation_accuracy*100:.2f}%, Validtion Loss: {validation_loss}\")\n",
    "    if validation_loss < best_valid_loss:\n",
    "        best_valid_loss = validation_loss\n",
    "        torch.save(model.state_dict(), 'ResNetMedium.pt')\n",
    "    val_acc_history_medium.append(validation_accuracy)\n",
    "    val_loss_history_medium.append(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "plt.title('Dynamic Learning Rate behaviour during the training of ResNetMedium Model')\n",
    "plt.plot(range(len(lr_tracker)), lr_tracker.values())\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "plt.title('Accuracy and Loss Plots for ResNetMedium Model')\n",
    "plt.plot(train_loss_history_medium, label='Train Loss')\n",
    "plt.plot(val_loss_history_medium, label='Val Loss')\n",
    "plt.plot(train_acc_history_medium, label='Train Acc')  \n",
    "plt.plot(val_acc_history_medium, label='Val Acc')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "yticks = np.linspace(0, 1.1, num=30)\n",
    "ax.set_yticks(yticks)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_true_medium, y_pred_medium)\n",
    "fig = plt.figure(figsize = (10, 10));\n",
    "ax = fig.add_subplot(1, 1, 1);\n",
    "ax.set_title('Confusion Matrix for ResNetMedium Model')\n",
    "cm = ConfusionMatrixDisplay(cm, display_labels = classes);\n",
    "cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)\n",
    "plt.xticks(rotation = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_min = 7*1e-5\n",
    "lr_max = 1e-2\n",
    "epochs = 30\n",
    "step_size = (len(train_dataset)/64) // 2\n",
    "\n",
    "model = modellarge\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr_min, momentum=0.9, nesterov=True)\n",
    "scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=lr_min, max_lr=lr_max, step_size_up=step_size, step_size_down=step_size, gamma=0.9999, mode=\"exp_range\", cycle_momentum=False)\n",
    "lr_tracker = {}\n",
    "\n",
    "train_loss_history_large = []\n",
    "train_acc_history_large = []\n",
    "val_loss_history_large = []\n",
    "val_acc_history_large = []\n",
    "y_pred_large = []\n",
    "y_true_large = []\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "    correct, loss, rate_tracker = train(data_loader=train_loader, model=model, criterion=criterion, optimizer=optimizer, scheduler=scheduler)\n",
    "    accuracy = correct / len(train_loader.dataset)\n",
    "    loss = loss / len(train_loader)\n",
    "    train_loss = loss\n",
    "    train_acc_history_large.append(accuracy)\n",
    "    train_loss_history_large.append(loss)\n",
    "    for key in rate_tracker.keys():\n",
    "        lr_tracker[(epoch,key)] = rate_tracker[key]\n",
    "    correct, loss, y_true_large, y_pred_large = evaluate(data_loader = test_loader, model=model, criterion=criterion)\n",
    "    validation_accuracy = correct / len(test_loader.dataset)\n",
    "    validation_loss = loss / len(test_loader)\n",
    "    print(f\"Train Accuracy: {accuracy*100:.2f}%, Train Loss: {train_loss}\")\n",
    "    print(f\"Validation Accuracy: {validation_accuracy*100:.2f}%, Validtion Loss: {validation_loss}\")\n",
    "    if validation_loss < best_valid_loss:\n",
    "        best_valid_loss = validation_loss\n",
    "        torch.save(model.state_dict(), 'ResNetCustomLarge.pt')\n",
    "    val_acc_history_large.append(validation_accuracy)\n",
    "    val_loss_history_large.append(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "plt.title('Dynamic Learning Rate behaviour during the training of ResNetMedium Model')\n",
    "plt.plot(range(len(lr_tracker)), lr_tracker.values())\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "plt.title('Accuracy and Loss Plots for ResNetMedium Model')\n",
    "plt.plot(train_loss_history_large, label='Train Loss')\n",
    "plt.plot(val_loss_history_large, label='Val Loss')\n",
    "plt.plot(train_acc_history_large, label='Train Acc')  \n",
    "plt.plot(val_acc_history_large, label='Val Acc')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "yticks = np.linspace(0, 1.1, num=30)\n",
    "ax.set_yticks(yticks)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_true_large, y_pred_large)\n",
    "fig = plt.figure(figsize = (10, 10));\n",
    "ax = fig.add_subplot(1, 1, 1);\n",
    "ax.set_title('Confusion Matrix for ResNetLarge Model')\n",
    "cm = ConfusionMatrixDisplay(cm, display_labels = classes);\n",
    "cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)\n",
    "plt.xticks(rotation = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
